{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b899393e-4b89-4c7a-aaec-d65d56ba7520",
   "metadata": {},
   "source": [
    "# First We Install Some Libraries\n",
    "The basic library if transformrers.  We also need accelerate for some reaon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d9c59f-db7c-4cac-a48d-8296cdaff37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.8/site-packages (4.27.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.local/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate) (5.5.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.3.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/lib/python3/dist-packages (from accelerate) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers \n",
    "!pip install accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77475b8c-fece-4e52-a6b8-f00e4b2c48f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae3a009d20f4ecf9545f31f66446ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\")     \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b0679-0211-4aea-8379-055fd388e352",
   "metadata": {},
   "source": [
    "This is a function that, given a query, generates a response from the LLM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b370253-f25a-4d72-9187-f82aa0228a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(str):\n",
    "    inputs = tokenizer(str, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs, max_length=500,\n",
    "                             num_beams=2,\n",
    "                             repetition_penalty=2.5,\n",
    "                             length_penalty=1.0,\n",
    "                             early_stopping=True,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             use_cache=True,\n",
    "                             do_sample = True,\n",
    "                             temperature = 1.5,\n",
    "                             top_k = 50,\n",
    "                             top_p = 0.95)\n",
    "    \n",
    "    print(tokenizer.decode(outputs[0][1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9927bf2-6faf-44d0-a561-522dfc8e3761",
   "metadata": {},
   "source": [
    "We now test if the model works.  This is tuned to answer questions with short responses, so it gives just a few words as the answer.  We use a pretty high temperature so the output often changes.  \n",
    "\n",
    "Sometimes it says *record sales* or *singer songwriter* or *he had long and bizarre hair* which I suppose are arguably correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a71eb63b-9b32-4820-af5d-d7446afd8c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "songwriter and musician\n"
     ]
    }
   ],
   "source": [
    "f(\"How did David Bowie become famous?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923f60f-bf8c-4659-aa55-661e0f1c3308",
   "metadata": {},
   "source": [
    "Now we install peft, the package that allows parameter efficient fine tuning.  We will also need datasets (to load a dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ef7aed-da5d-45d8-84b1-4c5223abda08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets==2.9.0 in /home/ubuntu/.local/lib/python3.8/site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (0.15.1)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (23.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets==2.9.0) (5.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets==2.9.0) (19.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (1.3.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.9.0) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.9.0) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets==2.9.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets==2.9.0) (2023.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.9.0) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install \"peft==0.2.0\" --quiet\n",
    "!pip install \"transformers==4.27.2\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" loralib  --quiet\n",
    "!pip install \"datasets==2.9.0\"\n",
    "\n",
    "!pip install protobuf==\"3.20.*\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c5cd98-32ee-44b3-8e0b-fbbb3587e0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/ubuntu/data/ exists\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "jjout = []\n",
    "with open(\"/home/ubuntu/five-dollar-test/train.json\") as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        i = json.loads(line)\n",
    "        if not i.get(\"input\") or not i.get(\"output\"):\n",
    "            continue\n",
    "        jjout.append({\"input\":  i[\"input\"], \"output\": i[\"output\"]})\n",
    "        \n",
    "import random\n",
    "random.shuffle(jjout)\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(\"/home/ubuntu/data/\")\n",
    "except:\n",
    "    print(\"Directory /home/ubuntu/data/ exists\")\n",
    "\n",
    "with open(\"/home/ubuntu/data/train.json\", \"w\") as file:\n",
    "    file.write(json.dumps(jjout[100:]))\n",
    "\n",
    "from datasets import load_dataset\n",
    "data_files = {\"train\": \"train.json\"}\n",
    "dataset = load_dataset(\"/home/ubuntu/data/\", data_files=data_files)\n",
    "save_dataset_path = \"/home/ubuntu/data-all-test\" \n",
    "\n",
    "max_length = 512 #tokenizer.model_max_length\n",
    "\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    inputs = [f\"{{input}}\\n\".format(input=item) for item in sample['input']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=padding, truncation=True)\n",
    "    labels = tokenizer(text_target=sample['output'], max_length=max_length, padding=padding, truncation=True)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))\n",
    "tokenized_dataset[\"train\"].save_to_disk(os.path.join(save_dataset_path,\"train\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffad8da-fb08-4321-8040-5083011751aa",
   "metadata": {},
   "source": [
    "We load our dataset from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ada9b77-925d-46e3-b00c-2ae9066ef3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk(\"/home/ubuntu/data-all-test/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469abc50-cfa8-43f0-9de6-c8a106ccc0a4",
   "metadata": {},
   "source": [
    "Now we make the lora model which is a wrapper around the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a60ef5-3039-4087-a3d4-dd30b95ba2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25165824 || all params: 19484779520 || trainable%: 0.12915631903439675\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    " r=16,\n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "import torch.nn as nn\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebaf3655-d1a5-461b-a8b3-415f5c36cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43cce517-cc0b-4e06-8cd6-1ee710d0e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"lora-flan-ul2-fact\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    #auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    max_steps=1000, # takes \n",
    "    bf16=True,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    "    optim='adamw_torch'\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00193d6-fca5-4092-b0a9-3329f92bcdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945f7ae-d58e-40ac-bb7e-6f8212cca9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  90/1000 01:41 < 17:24, 0.87 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.300200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b01670c0-2e98-4837-a875-27b7da58b019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a62e6-383d-4fa5-b403-b8e37feff636",
   "metadata": {},
   "source": [
    "This saves the peft model in results-ul2-1k.  Then it merges the peft model into the UL2 model to make a new model and saves it as merged-ul2-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309e7603-8ec9-45ad-b8c2-0106980196e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results-fact-ul2-1k-v2/tokenizer_config.json',\n",
       " 'results-fact-ul2-1k-v2/special_tokens_map.json',\n",
       " 'results-fact-ul2-1k-v2/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_id=\"results-ul2-1k\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "\n",
    "key_list = [key for key, _ in model.base_model.model.named_modules() if \"lora\" not in key]\n",
    "for key in key_list:\n",
    "    parent, target, target_name = model.base_model._get_submodules(key)\n",
    "    if isinstance(target, peft.tuners.lora.Linear):\n",
    "        bias = target.bias is not None\n",
    "        new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n",
    "        model.base_model._replace_module(parent, target_name, new_module, target)\n",
    "\n",
    "model = model.base_model.model\n",
    "model.save_pretrained(\"/home/ubuntu/merged-ul2-model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea083e2-36ce-4e00-a727-8d4ac5597f02",
   "metadata": {},
   "source": [
    "This is how to load the peft model.  It is faster to load the merged model however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2f8874-9f3b-48a3-9efb-cdde9e8d7283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea9b5307b3a481280926bb0a3d2032b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "tokenizer\n",
      "lora\n",
      "eval\n",
      "Peft model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: does the white house have a dining room\n",
      "------------------------------\n",
      "Answer:\n",
      "The White House dining room is one of the most famous rooms in Washington, D.C. It's where the president and first lady dine on a daily basis, and it has been home to many historic events since its construction in 1792. In addition to hosting dignitaries from around the world, the dining area also serves as an important ceremonial space for official dinners and luncheons. A few years ago, we took you behind the scenes of this iconic space with our \"Behind the Scenes\" series. Now, let us take you through the history of how the Dining Room became what it is today. As the story goes, President George Washington wanted his guests to be able to enjoy their meals without having to leave the building. He decided to build the first presidential dining hall at the north end of Pennsylvania Avenue, across from the U.S. Capitol Building. The design was inspired by Italian Renaissance palaces, including the Palazzo Vecchio in Florence, Italy. At the time, there were no kitchens in the main building, so the food had to either be brought in or cooked on site. That changed in 1912, when Congress passed legislation that allowed the executive mansion to have its own kitchen. This new kitchen would serve as the primary source of food for the entire household. Over the next several decades, additional kitchen buildings were added to the west side of Penn Avenue. One of them housed ice storage facilities, while another contained warewashing equipment. By the mid-1920s however, these structures were being used for other purposes, such as housing staff members. After World War II, more renovations were made to accommodate the growing number of people who worked at 1600 Pennsylvania Ave. On August 29, 1948, during the administration of President Harry S. Truman, Secretary of State Henry Kissinger announced that the United States government would purchase the property at 1601 PennsylvaniaAve. (The property had previously been owned by the National Park Service.) With the purchase came the right to renovate the existing structure, which included the creation of two new dining rooms. Construction began in 1950, but it wasn't completed until 1953. According to records, over $600,000 was spent on the project. Today, visitors can see the original wood paneling, marble floors, decorative ceiling molding, light fixtures, furniture, wallpaper, artwork, china cabinet, silver, glassware,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc.\n",
    "peft_model_id = \"results-f-ul2-1k-v2\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "print(\"config\")\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  device_map={\"\":0})\n",
    "print(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "print(\"tokenizer\")\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "print(\"lora\")\n",
    "model.eval()\n",
    "print(\"eval\")\n",
    "print(\"Peft model loaded\")\n",
    "\n",
    "def f(sample):\n",
    "    input_ids = tokenizer(sample, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    # with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids=input_ids, \n",
    "                             max_new_tokens=500, \n",
    "                             do_sample=True, \n",
    "                             top_p=0.9,\n",
    "                             num_beams=3,\n",
    "                             repetition_penalty=2.5,\n",
    "                             early_stopping=True,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             use_cache=True,\n",
    "                             top_k = 50)\n",
    "    print(f\"input sentence: {sample}\\n{'---'* 10}\")\n",
    "    print(f\"Answer:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")\n",
    "\n",
    "f(\"does the white house have a dining room\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bbb9fa0-2ea7-4c82-87c7-29b62cc8b049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: Does the white house have a pool\n",
      "------------------------------\n",
      "Answer:\n",
      "The White House has a pool, but it's not exactly what you'd call an \"infinity pool.\" In fact, the pool isn't even in the house itself. Instead, it sits on the grounds of the Washington Monument and is accessible to the public through the National Park Service. While some people might be surprised by the location of this swimming hole, others will be familiar with its history. The pool was originally built in 1812 as part of President Thomas Jefferson'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer =AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/home/ubuntu/merged-ul2-model\",torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "def f(sample):\n",
    "    input_ids = tokenizer(sample, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    # with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids=input_ids, \n",
    "                             max_new_tokens=500, \n",
    "                             do_sample=True, \n",
    "                             top_p=0.9,\n",
    "                             num_beams=3,\n",
    "                             repetition_penalty=2.5,\n",
    "                             early_stopping=True,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             use_cache=True,\n",
    "                             top_k = 50)\n",
    "    print(f\"input sentence: {sample}\\n{'---'* 10}\")\n",
    "    print(f\"Answer:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")\n",
    "\n",
    "f(\"does the white house have a dining room\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388c238e-8d9e-4aed-88d4-ed5dbbe855be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: what can newborns see\n",
      "------------------------------\n",
      "Answer:\n",
      "During infancy, infants have limited vision. Their eyes are closed and they can only see black and white images. When their eyes open at around four months of age, they begin to see color. However, it is not until the first year of life that they will fully develop their ability to distinguish colors. In fact, newborns may be able to recognize red and green but not blue or purple. A baby's sense of sight begins developing shortly after birth. The eye has two parts: the retina (the light-sensitive layer) and the optic nerve (which connects the eye to the brain). The retina converts light into electrical impulses that are sent to your brain. This process is called photoreception. Your brain then interprets these signals as an image. There are three stages of visual development during childhood: prenatal, early childhood, and late childhood. Prenatal development occurs before a baby is born. It includes the formation of the embryonic eye and its placement within the body. After birth, there is no further development of this stage. Early childhood development takes place between one and six years old. At this point, the child learns how to use his or her eyes to focus on objects near and far away. By the time he or she is seven or eight yearsold, most children have developed some degree of binocular vision, which means that both eyes work together to help them see things better. Late childhood refers to when children are between seven and 12 years of aged. As children get older, their central nervous system continues to develop and improve. They also become more aware of their surroundings. For example, by the age of five or six, many children can tell the difference between day and night. Some children even understand the meaning of words such as \"yes\" and \"no.\" Most children do not reach full adult vision until about age 20.\n"
     ]
    }
   ],
   "source": [
    "f(\"what can newborns see\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2132690c-b7e7-47bc-9c00-ff96063b0cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: how can the ukraine win the war\n",
      "------------------------------\n",
      "Answer:\n",
      "The war in eastern Ukraine has been going on for over a year now, and both sides have suffered significant casualties. While the Ukrainian government claims to be winning, it is not clear that this is the case. According to the United Nations High Commissioner for Refugees (UNHCR), as of December 1, 2018, more than 11,000 people had been killed in the conflict, with an additional 33,000 wounded. In addition, there are nearly 200,000 internally displaced persons (IDPs) who have been forced to leave their homes due to violence or lack of humanitarian aid. A recent report by the UN Special Rapporteur on the situation of IDPs in Ukraine, Nadine El-Baghdadi, concluded that \"the number of civilian deaths and injuries will continue to rise\" unless immediate action is taken to address the crisis.\n"
     ]
    }
   ],
   "source": [
    "f(\"how can the ukraine win the war\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f8992-8b45-43f9-8fd2-aa062c6c29b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b11f8c-3eef-4397-9161-d4034266379f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
