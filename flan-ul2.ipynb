{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b899393e-4b89-4c7a-aaec-d65d56ba7520",
   "metadata": {},
   "source": [
    "# First We Install Some Libraries\n",
    "The basic library is transformrers.  We also need accelerate for some reason. This takes 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d9c59f-db7c-4cac-a48d-8296cdaff37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.8/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install \"numexpr==2.7.3\" --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bc3fce-cd61-453d-a219-b27257767a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade fsspec --quiet\n",
    "!pip install \"datasets==2.9.0\" --quiet\n",
    "!pip install streamlit --quiet\n",
    "!pip install protobuf==\"3.20.*\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9866f3c-9c61-4b54-9614-f14432a51e78",
   "metadata": {},
   "source": [
    "### Downloading the model from HuggingFace\n",
    "This loads the Flan-UL2 model and its tokenizer.  This will download the model from Huggngface so will take 15 minutes as the model is 40GB. The next time we run this, it will be cached. The second time we run it will be faster, about 25 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77475b8c-fece-4e52-a6b8-f00e4b2c48f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9551f37a4284431891605c3f864ad87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\")     \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b0679-0211-4aea-8379-055fd388e352",
   "metadata": {},
   "source": [
    "## Our generate function\n",
    "This is a function that, given a query, generates a response from the LLM. This is for testing. Later we will turn this into a Streamlit app.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b370253-f25a-4d72-9187-f82aa0228a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(str):\n",
    "    inputs = tokenizer(str, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs, max_length=500,\n",
    "                             num_beams=2,\n",
    "                             repetition_penalty=2.5,\n",
    "                             length_penalty=1.0,\n",
    "                             early_stopping=True,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             use_cache=True,\n",
    "                             do_sample = True,\n",
    "                             temperature = 1.5,\n",
    "                             top_k = 50,\n",
    "                             top_p = 0.95)\n",
    "    \n",
    "    print(tokenizer.decode(outputs[0][1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9927bf2-6faf-44d0-a561-522dfc8e3761",
   "metadata": {},
   "source": [
    "## Testing the original Flan-UL2\n",
    "We now test if the model works.  This is tuned to answer questions with short responses, so it gives just a few words as the answer.  We use a pretty high temperature so the output often changes.  \n",
    "\n",
    "Sometimes it says *record sales* or *singer songwriter* or *he had long and bizarre hair* which I suppose are arguably correct. These answers are really short, and we are going to fine tune things to make it give long, web-page like answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71eb63b-9b32-4820-af5d-d7446afd8c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in music\n"
     ]
    }
   ],
   "source": [
    "f(\"How did David Bowie become famous?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923f60f-bf8c-4659-aa55-661e0f1c3308",
   "metadata": {},
   "source": [
    "## Install Peft and other libraries for fine tunung\n",
    "\n",
    "Now we install peft, the package that allows parameter efficient fine tuning.  We will also need datasets (to load a dataset). This takes 12 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ef7aed-da5d-45d8-84b1-4c5223abda08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install \"peft==0.2.0\" --quiet\n",
    "!pip install \"transformers==4.27.2\"  \"accelerate==0.17.1\" \"evaluate==0.4.0\" loralib  --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752cb4b-8cab-4ef3-a300-c1a3540e427f",
   "metadata": {},
   "source": [
    "## Where we keep things\n",
    "\n",
    "We set a bunch of variables so we have names of where we keep some files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f693820c-7f9a-45b4-b657-7ec5c455a6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /home/ubuntu/five-dollar-test/what.json /home/ubuntu/five-dollar-test/how.json >  /home/ubuntu/five-dollar-test/train.json\n",
    "training_data = \"/home/ubuntu/five-dollar-test/train.json\" # The training data is taken from this file.\n",
    "shuffled_data = \"/home/ubuntu/data/train.json\" # We shuffle the data and put it here.\n",
    "tokenized_data = \"/home/ubuntu/data-all-test/train\"  # Where we put the tokenized data\n",
    "\n",
    "output_dir=\"/home/ubuntu/lora-flan-ul2-fact\"  # the place where we save the interim results\n",
    "\n",
    "peft_model_id=\"/home/ubuntu/results-ul2-1k\"  # The place where we save the peft model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c20db-14c7-446f-b82e-874a22c402b5",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "We load the data in, shuffle it, and write it out as tokens (so numbers). This takes about 50 seconds, so we save the data so we can reload it quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88c5cd98-32ee-44b3-8e0b-fbbb3587e0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-db8196adcd8bcd5e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/data to /home/ubuntu/.cache/huggingface/datasets/json/data-db8196adcd8bcd5e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3f8e102d364b2ea84a43b04a3fe15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669c3b8227e4481dbda3badd8ba30e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1930b0ca77141cfb22c5e3d2e6d15ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/data-db8196adcd8bcd5e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e43e9f3467f4d73a6103cbbfaeb922f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4d70814fab4d5d828a6f782d73b547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2633538de849aa823963fc6273a1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "jjout = []\n",
    "with open(training_data ) as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        i = json.loads(line)\n",
    "        if not i.get(\"input\") or not i.get(\"output\"):\n",
    "            continue\n",
    "        jjout.append({\"input\":  i[\"input\"], \"output\": i[\"output\"]})\n",
    "        \n",
    "import random\n",
    "random.shuffle(jjout)  # randomly shuffle the data.\n",
    "\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(\"/home/ubuntu/data/\")\n",
    "except:\n",
    "    print(\"Directory /home/ubuntu/data/ exists\")\n",
    "\n",
    "with open(shuffled_data, \"w\") as file:\n",
    "    file.write(json.dumps(jjout))\n",
    "\n",
    "from datasets import load_dataset\n",
    "data_files = {\"train\": \"train.json\"}\n",
    "dataset = load_dataset(\"/home/ubuntu/data/\", data_files=data_files)\n",
    "\n",
    "max_length = 512 #tokenizer.model_max_length  Flan-ul2 has this set to 2048 which makes things a little slow.  If we used flash attention this would be quicker.\n",
    "\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    inputs = [f\"{{input}}\\n\".format(input=item) for item in sample['input']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=padding, truncation=True)\n",
    "    labels = tokenizer(text_target=sample['output'], max_length=max_length, padding=padding, truncation=True)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))\n",
    "tokenized_dataset[\"train\"].save_to_disk(tokenized_data )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffad8da-fb08-4321-8040-5083011751aa",
   "metadata": {},
   "source": [
    "## Load back in the datset\n",
    "\n",
    "We load our dataset from disk.  This is mostly so we can rerun things without regenerating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ada9b77-925d-46e3-b00c-2ae9066ef3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk(tokenized_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469abc50-cfa8-43f0-9de6-c8a106ccc0a4",
   "metadata": {},
   "source": [
    "## Making the LoRA model\n",
    "\n",
    "Now we make the lora model which is a wrapper around the base model. This takes 25 seconds.  Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a60ef5-3039-4087-a3d4-dd30b95ba2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Trainable Parameters!!\n",
      "trainable params: 12582912 || all params: 19472196608 || trainable%: 0.06461988985274732\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    " r=8,  # the rank.  This is the smaller dimension of the two d*r and r*d matrices that multiply to make a d*d matrix.\n",
    " lora_alpha=32,  # this is the mixing factor.  32 is quite high.\n",
    " target_modules=[\"q\", \"v\"],  # We just replace the value and query matrices that are used in attention. The choices are  [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"].\n",
    " lora_dropout=0.05,  # This is a regularization thing to make the model train better.  We ignore 5% of the parameters each time to encourage the others.\n",
    " bias=\"none\", # This can be none, lora_only, or all.  none seems the safe choice.\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "import torch.nn as nn\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # This makes things faster\n",
    "model.enable_input_require_grads() # This computes the gradients for the input embeddings so we can fine tune.\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)  # we loaded the base model in bf16 to we need to cast the top to float so we can peft it.\n",
    "model.config.use_cache = False  # You can't use the cache with gradient checkpointing.  \n",
    "\n",
    "print(\"The Trainable Parameters!!\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130c0cf-c3ba-4457-a3f5-3c0481af3f75",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf3655-d1a5-461b-a8b3-415f5c36cc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0399db37-816f-4281-a1d5-1e88ee3ceb03",
   "metadata": {},
   "source": [
    "# The Training Parameters\n",
    "\n",
    "These are what controls the trainer. The data collator helps load the data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cce517-cc0b-4e06-8cd6-1ee710d0e5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8, # We train 8 examples at a time.\n",
    "    gradient_accumulation_steps=1,  # We don't accumulate gradients\n",
    "    #auto_find_batch_size=True,  # This sometimes works, and can replace the previous two lines, but gave me out of mempoery errors once.\n",
    "    learning_rate=1e-3, # higher learning rate,  Learning rates are usually 10 or 100 times smaller than this, but this seems to work.\n",
    "    num_train_epochs=1, # We train for 1 epoch, but actually we stop after max_steps so this will not be ineffect unless you have less than 1000 examples.\n",
    "    logging_dir=f\"{output_dir}/logs\", # Where we log things\n",
    "    logging_strategy=\"steps\", # We log by steps.  epoch would loh once every epoch and no turns off logging.\n",
    "    logging_steps=50,  # we log every 50 steps to see how things are going.\n",
    "    max_steps=250, # take 250 steps of 8 examples, so 1000 examples in total.\n",
    "    bf16=True,  # We train in bf16 as fp16 can overflow in T5 models.  bf16 has more range but less precision.\n",
    "    save_strategy=\"no\", #\n",
    "    report_to=\"tensorboard\",  # This makes our little progress bar\n",
    "    optim='adamw_torch',  # this is the AdamW optimizer as the default one is deprecated.\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00193d6-fca5-4092-b0a9-3329f92bcdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df83d23-6290-457d-ba69-d54b0508a172",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "This takes 10 minutes or so.  If you make max_steps bigger, it will take longer.  Quality will change, but 1000 samples seems to work well.\n",
    "Remember we are trying to teach it a style, not to teach it facts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5945f7ae-d58e-40ac-bb7e-6f8212cca9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 08:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.214100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.212000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=2.2277659606933593, metrics={'train_runtime': 533.386, 'train_samples_per_second': 3.75, 'train_steps_per_second': 0.469, 'total_flos': 1.18828642074624e+17, 'train_loss': 2.2277659606933593, 'epoch': 0.24})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01670c0-2e98-4837-a875-27b7da58b019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae2a62e6-383d-4fa5-b403-b8e37feff636",
   "metadata": {},
   "source": [
    "## Save the Peft Model\n",
    "This saves the peft model in results-ul2-1k or whatever you set *peft_model_id* to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216d3b4e-381b-4de0-a4fc-62139503a563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/results-ul2-1k/tokenizer_config.json',\n",
       " '/home/ubuntu/results-ul2-1k/special_tokens_map.json',\n",
       " '/home/ubuntu/results-ul2-1k/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced882f-f78b-444b-a7a8-5c0fa5a8fefb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge the Peft model with the original model\n",
    "We merge the models together so that inference will be faster. This takes about two minutes.   Once this is done, we restart the kernel.  This will prevent us running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c95f59-59d5-40f3-b733-0c784ae03915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "803c9b7b-03e1-4c77-98ff-5ad798dd2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "saving\n",
      "done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8337ec5b-a7dc-43de-9199-e798ed16af70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ded95e1fab4273aa1799cc47c51d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "tokenizer\n",
      "lora\n",
      "merge\n",
      "reset\n",
      "saving\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "#from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "print(\"load model\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\")     \n",
    "\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16,  device_map={\"\":0})\n",
    "print(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\" )\n",
    "print(\"tokenizer\")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "print(\"lora\")\n",
    "model.eval()\n",
    "print(\"merge\")\n",
    "\n",
    "import torch.nn as nn\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "import peft \n",
    "key_list = [key for key, _ in model.base_model.model.named_modules() if \"lora\" not in key]\n",
    "for key in key_list:\n",
    "    parent, target, target_name = model.base_model._get_submodules(key)\n",
    "    if isinstance(target, peft.tuners.lora.Linear):\n",
    "        bias = target.bias is not None\n",
    "        new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n",
    "        model.base_model._replace_module(parent, target_name, new_module, target)\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "print(\"reset\")\n",
    "model = model.base_model.model\n",
    "print(\"saving\")\n",
    "model.save_pretrained(\"/home/ubuntu/merged-ul2-model-v4\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48819764-6207-437e-a77e-813eb05ce055",
   "metadata": {},
   "source": [
    "## Start our Streamlit App\n",
    "\n",
    "First, we restart the kernel, choosing \"Restart Kernel\" from the Kernel menu above.  This is to save GPU memory, which we will need for the Streamlit app.  Then we start the streamlit app with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1c156-eb87-4799-af36-fd1517481998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.29.150.84:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://209.20.159.85:8501\u001b[0m\n",
      "\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:22<00:00,  5.75s/it]\n",
      "2023-06-04 22:12:17.908541: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-04 22:12:18.113032: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: No preset parameters were found for the device that Open MPI\n",
      "detected:\n",
      "\n",
      "  Local host:            209-20-159-85\n",
      "  Device name:           mlx5_0\n",
      "  Device vendor ID:      0x02c9\n",
      "  Device vendor part ID: 4122\n",
      "\n",
      "Default device parameters will be used, which may result in lower\n",
      "performance.  You can edit any of the files specified by the\n",
      "btl_openib_device_param_files MCA parameter to set values for your\n",
      "device.\n",
      "\n",
      "NOTE: You can turn off this warning by setting the MCA parameter\n",
      "      btl_openib_warn_no_device_params_found to 0.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           209-20-159-85\n",
      "  Local device:         mlx5_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       udcm\n",
      "--------------------------------------------------------------------------\n",
      "input sentence: does the white house have a pool\n",
      "------------------------------\n",
      "\n",
      "2023-06-04 22:12:36,461\n",
      "\n",
      "            Question: does the white house have a pool\n",
      "            Answer: Answer:\n",
      "If you've ever been to the White House, you know that there is a swimming pool. It's on the side of the South Building, and while it may not look like much from the outside, it has something going for it: its history. Like many things in Washington, D.C., this one has some interesting stories behind it. Read on to learn about the story of The Whitehouse Pool! Before we get started, I should point out that although the pool was once known as \"The Blue Pool,\" it does not include blue water. Instead, the name refers to an unidentified color or shades of blue that can be seen throughout the summer season. That being said, here are some fun facts about our very own pool! The original idea for The Blue Bathtub came from none other than President Abraham Lincoln. In 1865, Lincoln invited his son, Emancipation Proclamation signer Robert Lincoln, to join him for dinner at the Oval Office. As they were eating their meal, he noticed how beautiful the reflection of trees looked in the water (it was also near dusk) and wondered what would happen if someone tried to jump into the bathtub. He decided to take matters into his own hands and asked for permission to install wrought iron railings along the south side windows so that people could see the view without getting wet. This was done in 1866, making it the first time that anyone had attempted to put fencing around the bath tub. After years of use, by 1910, several fences had been added to prevent swimmers from jumping off the edge. By 1926, four additional fence panels were installed onto two ends of each window. Today, three of them still remain. Over the years, various attempts have been made to add fountains and lights to improve the overall appearance. But since those additions weren't permanent, they eventually fell out of favor with the staff. A few weeks after World War II ended, Franklin Delano Roosevelt took over the presidency. With the end of war-related tensions, work began on beautifying the grounds surrounding the building. One thing that FDR wanted to do was replace the old wooden decked steps leading up to Old Glory. However, because the concrete floor was damaged during the war, no one knew where to start. Enter Thomas Edison, who suggested digging up the entire area and replacing it with\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "\n",
    "!streamlit run app.py -- --merged_model /home/ubuntu/merged-ul2-model-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb454b6-4c26-43a1-9b95-cb883d08c583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6dcdeb-a857-4845-8bf8-39fd9dd5f679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
