{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b899393e-4b89-4c7a-aaec-d65d56ba7520",
   "metadata": {},
   "source": [
    "# First We Install Some Libraries\n",
    "The basic library is transformrers.  We also need accelerate for some reason. This takes 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d9c59f-db7c-4cac-a48d-8296cdaff37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.8/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install \"numexpr==2.7.3\" --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bc3fce-cd61-453d-a219-b27257767a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade fsspec --quiet\n",
    "!pip install \"datasets==2.9.0\" --quiet\n",
    "!pip install streamlit --quiet\n",
    "!pip install protobuf==\"3.20.*\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9866f3c-9c61-4b54-9614-f14432a51e78",
   "metadata": {},
   "source": [
    "### Downloading the model from HuggingFace\n",
    "This loads the Flan-UL2 model and its tokenizer.  This will download the model from Huggngface so will take 15 minutes as the model is 40GB. The next time we run this, it will be cached. The second time we run it will be faster, about 25 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77475b8c-fece-4e52-a6b8-f00e4b2c48f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2149ab24614449d1af70b3e10ed3389e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\")     \n",
    "orig_model = model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b0679-0211-4aea-8379-055fd388e352",
   "metadata": {},
   "source": [
    "## Our generate function\n",
    "This is a function that, given a query, generates a response from the LLM. This is for testing. Later we will turn this into a Streamlit app.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b370253-f25a-4d72-9187-f82aa0228a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(str):\n",
    "    inputs = tokenizer(str, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs, max_length=500,\n",
    "                             penalty_alpha=0.6,\n",
    "                             top_k = 8,\n",
    "                             repetition_penalty=2.5,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             )\n",
    "    \n",
    "    print(tokenizer.decode(outputs[0][1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9927bf2-6faf-44d0-a561-522dfc8e3761",
   "metadata": {},
   "source": [
    "## Testing the original Flan-UL2\n",
    "We now test if the model works.  This is tuned to answer questions with short responses, so it gives just a few words as the answer.  \n",
    "Sometimes it says *he was singer* which I suppose are arguably correct. These answers are really short, and we are going to fine tune things to make it give long, web-page like answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a71eb63b-9b32-4820-af5d-d7446afd8c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he was singer\n"
     ]
    }
   ],
   "source": [
    "f(\"How did David Bowie become famous?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923f60f-bf8c-4659-aa55-661e0f1c3308",
   "metadata": {},
   "source": [
    "## Install Peft and other libraries for fine tunung\n",
    "\n",
    "Now we install peft, the package that allows parameter efficient fine tuning.  We will also need datasets (to load a dataset). This takes 12 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ef7aed-da5d-45d8-84b1-4c5223abda08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"peft==0.2.0\" --quiet\n",
    "!pip install \"transformers==4.27.2\"  \"accelerate==0.17.1\" \"evaluate==0.4.0\" loralib  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752cb4b-8cab-4ef3-a300-c1a3540e427f",
   "metadata": {},
   "source": [
    "## Where we keep things\n",
    "\n",
    "We set a bunch of variables so we have names of where we keep some files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f693820c-7f9a-45b4-b657-7ec5c455a6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /home/ubuntu/five-dollar-test/what.json /home/ubuntu/five-dollar-test/how.json >  /home/ubuntu/five-dollar-test/train.json\n",
    "training_data = \"/home/ubuntu/five-dollar-test/train.json\" # The training data is taken from this file.\n",
    "shuffled_data = \"/home/ubuntu/data/train.json\" # We shuffle the data and put it here.\n",
    "tokenized_data = \"/home/ubuntu/data-all-test/train\"  # Where we put the tokenized data\n",
    "output_dir=\"/home/ubuntu/lora-flan-ul2-fact\"  # the place where we save the interim results\n",
    "peft_model_id=\"/home/ubuntu/results-ul2-1k\"  # The place where we save the peft model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c20db-14c7-446f-b82e-874a22c402b5",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "We load the data in, shuffle it, and write it out as tokens (so numbers). This takes about 50 seconds, so we save the data so we can reload it quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88c5cd98-32ee-44b3-8e0b-fbbb3587e0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-b4e9f2e56866c7ca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/data to /home/ubuntu/.cache/huggingface/datasets/json/data-b4e9f2e56866c7ca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deea15c85cd74182bd4430f70541519c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1916b51d886b473aa3ed58a4ae6edddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b6e3ae08a64df0abc517cb5f518b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/data-b4e9f2e56866c7ca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b97ba4d5bc47049d557538f6093653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd53dd7787a841ad991f420791e50adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ef17e9be794fe8956b6d5d56b2b1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "jjout = []\n",
    "with open(training_data ) as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        i = json.loads(line)\n",
    "        if not i.get(\"input\") or not i.get(\"output\"):\n",
    "            continue\n",
    "        jjout.append({\"input\":  i[\"input\"], \"output\": i[\"output\"]})\n",
    "        \n",
    "import random\n",
    "random.shuffle(jjout)  # randomly shuffle the data.\n",
    "\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(\"/home/ubuntu/data/\")\n",
    "except:\n",
    "    print(\"Directory /home/ubuntu/data/ exists\")\n",
    "\n",
    "with open(shuffled_data, \"w\") as file:\n",
    "    file.write(json.dumps(jjout))\n",
    "\n",
    "from datasets import load_dataset\n",
    "data_files = {\"train\": \"train.json\"}\n",
    "dataset = load_dataset(\"/home/ubuntu/data/\", data_files=data_files)\n",
    "\n",
    "max_length = 512 #tokenizer.model_max_length  Flan-ul2 has this set to 2048 which makes things a little slow.  If we used flash attention this would be quicker.\n",
    "\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    inputs = [f\"{{input}}\\n\".format(input=item) for item in sample['input']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=padding, truncation=True)\n",
    "    labels = tokenizer(text_target=sample['output'], max_length=max_length, padding=padding, truncation=True)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))\n",
    "tokenized_dataset[\"train\"].save_to_disk(tokenized_data )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffad8da-fb08-4321-8040-5083011751aa",
   "metadata": {},
   "source": [
    "## Load back in the datset\n",
    "\n",
    "We load our dataset from disk.  This is mostly so we can rerun things without regenerating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ada9b77-925d-46e3-b00c-2ae9066ef3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk(tokenized_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469abc50-cfa8-43f0-9de6-c8a106ccc0a4",
   "metadata": {},
   "source": [
    "## Making the LoRA model\n",
    "\n",
    "Now we make the lora model which is a wrapper around the base model. This takes 25 seconds.  Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a60ef5-3039-4087-a3d4-dd30b95ba2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Trainable Parameters!!\n",
      "trainable params: 12582912 || all params: 19472196608 || trainable%: 0.06461988985274732\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    " r=8,  # the rank.  This is the smaller dimension of the two d*r and r*d matrices that multiply to make a d*d matrix.\n",
    " lora_alpha=32,  # this is the mixing factor.  32 is quite high.\n",
    " target_modules=[\"q\", \"v\"],  # We just replace the value and query matrices that are used in attention. The choices are  [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"].\n",
    " lora_dropout=0.05,  # This is a regularization thing to make the model train better.  We ignore 5% of the parameters each time to encourage the others.\n",
    " bias=\"none\", # This can be none, lora_only, or all.  none seems the safe choice.\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "import torch.nn as nn\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # This makes things faster\n",
    "model.enable_input_require_grads() # This computes the gradients for the input embeddings so we can fine tune.\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)  # we loaded the base model in bf16 to we need to cast the top to float so we can peft it.\n",
    "model.config.use_cache = False  # You can't use the cache with gradient checkpointing.  \n",
    "\n",
    "print(\"The Trainable Parameters!!\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130c0cf-c3ba-4457-a3f5-3c0481af3f75",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf3655-d1a5-461b-a8b3-415f5c36cc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0399db37-816f-4281-a1d5-1e88ee3ceb03",
   "metadata": {},
   "source": [
    "# The Training Parameters\n",
    "\n",
    "These are what controls the trainer. The data collator helps load the data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43cce517-cc0b-4e06-8cd6-1ee710d0e5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8, # We train 8 examples at a time.\n",
    "    gradient_accumulation_steps=1,  # We don't accumulate gradients\n",
    "    #auto_find_batch_size=True,  # This sometimes works, and can replace the previous two lines, but gave me out of mempoery errors once.\n",
    "    learning_rate=1e-3, # higher learning rate,  Learning rates are usually 10 or 100 times smaller than this, but this seems to work.\n",
    "    num_train_epochs=1, # We train for 1 epoch, but actually we stop after max_steps so this will not be ineffect unless you have less than 1000 examples.\n",
    "    logging_dir=f\"{output_dir}/logs\", # Where we log things\n",
    "    logging_strategy=\"steps\", # We log by steps.  epoch would loh once every epoch and no turns off logging.\n",
    "    logging_steps=50,  # we log every 50 steps to see how things are going.\n",
    "    max_steps=250, # take 250 steps of 8 examples, so 2000 examples in total.\n",
    "    bf16=True,  # We train in bf16 as fp16 can overflow in T5 models.  bf16 has more range but less precision.\n",
    "    save_strategy=\"no\", #\n",
    "    report_to=\"tensorboard\",  # This makes our little progress bar\n",
    "    optim='adamw_torch',  # this is the AdamW optimizer as the default one is deprecated.\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00193d6-fca5-4092-b0a9-3329f92bcdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df83d23-6290-457d-ba69-d54b0508a172",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "This takes 10 minutes or so.  If you make max_steps bigger, it will take longer.  Quality will change, but 2000 samples seems to work well.\n",
    "Remember we are trying to teach it a style, not to teach it facts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5945f7ae-d58e-40ac-bb7e-6f8212cca9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 20:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.289200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.230700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=2.2465633850097655, metrics={'train_runtime': 1205.5337, 'train_samples_per_second': 1.659, 'train_steps_per_second': 0.207, 'total_flos': 1.18828642074624e+17, 'train_loss': 2.2465633850097655, 'epoch': 0.24})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01670c0-2e98-4837-a875-27b7da58b019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae2a62e6-383d-4fa5-b403-b8e37feff636",
   "metadata": {},
   "source": [
    "## Save the Peft Model\n",
    "This saves the peft model in results-ul2-1k or whatever you set *peft_model_id* to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "216d3b4e-381b-4de0-a4fc-62139503a563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/results-ul2-1k/tokenizer_config.json',\n",
       " '/home/ubuntu/results-ul2-1k/special_tokens_map.json',\n",
       " '/home/ubuntu/results-ul2-1k/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced882f-f78b-444b-a7a8-5c0fa5a8fefb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge the Peft model with the original model\n",
    "We merge the models together so that inference will be faster. This takes about two minutes.   Once this is done, we restart the kernel.  This will prevent us running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8337ec5b-a7dc-43de-9199-e798ed16af70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer\n",
      "lora\n",
      "merge\n",
      "reset\n",
      "saving\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "model = orig_model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\" )\n",
    "print(\"tokenizer\")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "print(\"lora\")\n",
    "model.eval()\n",
    "print(\"merge\")\n",
    "\n",
    "import torch.nn as nn\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "import peft \n",
    "key_list = [key for key, _ in model.base_model.model.named_modules() if \"lora\" not in key]\n",
    "for key in key_list:\n",
    "    parent, target, target_name = model.base_model._get_submodules(key)\n",
    "    if isinstance(target, peft.tuners.lora.Linear):\n",
    "        bias = target.bias is not None\n",
    "        new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n",
    "        model.base_model._replace_module(parent, target_name, new_module, target)\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model = model.base_model.model\n",
    "\n",
    "print(\"saving\")\n",
    "model.save_pretrained(\"/home/ubuntu/merged-ul2-model-v4\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48819764-6207-437e-a77e-813eb05ce055",
   "metadata": {},
   "source": [
    "## Start our Streamlit App\n",
    "\n",
    "First, we restart the kernel, choosing \"Restart Kernel\" from the Kernel menu above.  This is to save GPU memory, which we will need for the Streamlit app.  Then we start the streamlit app with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1c156-eb87-4799-af36-fd1517481998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.29.150.107:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://209.20.158.236:8501\u001b[0m\n",
      "\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:22<00:00,  5.70s/it]\n",
      "2023-06-16 20:55:06.619222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-16 20:55:06.918417: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: No preset parameters were found for the device that Open MPI\n",
      "detected:\n",
      "\n",
      "  Local host:            209-20-158-236\n",
      "  Device name:           mlx5_0\n",
      "  Device vendor ID:      0x02c9\n",
      "  Device vendor part ID: 4122\n",
      "\n",
      "Default device parameters will be used, which may result in lower\n",
      "performance.  You can edit any of the files specified by the\n",
      "btl_openib_device_param_files MCA parameter to set values for your\n",
      "device.\n",
      "\n",
      "NOTE: You can turn off this warning by setting the MCA parameter\n",
      "      btl_openib_warn_no_device_params_found to 0.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           209-20-158-236\n",
      "  Local device:         mlx5_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       udcm\n",
      "--------------------------------------------------------------------------\n",
      "input sentence: how do you do a blowout\n",
      "------------------------------\n",
      "\n",
      "2023-06-16 20:55:28,437\n",
      "\n",
      "            Question: how do you do a blowout\n",
      "            Answer: Answer:\n",
      "Blowouts are a great way to get your hair straight and smooth, but they can be expensive. If you want to do it yourself at home, there are several ways to achieve the same results without spending tens of dollars on specialized blowdryers or salon services. Here's how to make blown out hair at Home using three different methods: *: Use t-shirt fabric as blotting paper for wet hair *; Use an old pillowcase (or other soft material) to dry hair with * The first method is one of the most popular ways people use to blow out their hair. It works best on damp hair that has been washed and towel dried. You can use shirts made from cotton, linen, silk, or any other natural fiber. To start, take 1-2 pieces of shirt fabric and fold them into squares. Place them in clumps around your head so that they cover your entire scalp. Leave them like this until youre ready to style your locks. When your time comes, remove the ties and let your curls fall naturally. This method works well for short hair because it doesnt require much work. However, long hair may need more than two layers of twisted strands to keep them from falling flat. For longer hair, try wrapping each layer around 3-5 times before leaving them alone. After about 20 minutes, pull the ends of your curly hair up and away from your face. Repeat this process until all of our hair is completely dry. Once your done, run your fingers through your waves to separate them.\n",
      "            \n",
      "\n",
      "2023-06-16 20:55:36,456\n",
      "\n",
      "            Question: how do you do a blowout\n",
      "            Answer: Answer:\n",
      "Blowouts are a great way to get your hair straight and smooth, but they can be expensive. If you want to do it yourself at home, there are several ways to achieve the same results without spending tens of dollars on specialized blowdryers or salon services. Here's how to make blown out hair at Home using three different methods: *: Use t-shirt fabric as blotting paper for wet hair *; Use an old pillowcase (or other soft material) to dry hair with * The first method is one of the most popular ways people use to blow out their hair. It works best on damp hair that has been washed and towel dried. You can use shirts made from cotton, linen, silk, or any other natural fiber. To start, take 1-2 pieces of shirt fabric and fold them into squares. Place them in clumps around your head so that they cover your entire scalp. Leave them like this until youre ready to style your locks. When your time comes, remove the ties and let your curls fall naturally. This method works well for short hair because it doesnt require much work. However, long hair may need more than two layers of twisted strands to keep them from falling flat. For longer hair, try wrapping each layer around 3-5 times before leaving them alone. After about 20 minutes, pull the ends of your curly hair up and away from your face. Repeat this process until all of our hair is completely dry. Once your done, run your fingers through your waves to separate them.\n",
      "            \n",
      "input sentence: how did jimi hendrix die\n",
      "------------------------------\n",
      "\n",
      "2023-06-16 20:56:16,603\n",
      "\n",
      "            Question: how did jimi hendrix die\n",
      "            Answer: Answer:\n",
      "The Jimi Hendrix Experience was an American rock band formed in Seattle, Washington in 1967. Its lineup consisted of guitarist Jim Morrison, singer/songwriter Jim Leverton and bassist Noel Redding. They were known for their innovative music style, which combined elements of rock, blues, jazz, psychedelic rock and experimental music. Their best-known songs include \"Purple Haze\", \"Foxy Lady\", the title track to their debut album, \"Are You Experienced\", and \"Crosstown Traffic\". In 1969, after three years of touring, the band broke up. After several years without recording or performing, they reunited in 1970 with new members Billy Corgan (guitar) and Gregg Rolie (bass). They recorded two albums, Are You Experienced and Electric Ladyland ; both reached number one on the Billboard 200 chart. However, by the end of the 1970s it became clear that the group had lost its original momentum. As a result, lead singer Jim Morrison left the following year, leaving Leverentz as the sole remaining original member. By the late 1970's the bands popularity had declined significantly, and in 1980, drummer Mitch Mitchell left to form his own band, Blue Cheer!. Although the remainder of this line-up continued to tour until 1982, there was no official reunion. On May 27, 2017, at the age of 70, Jimmi Hendrx died from complications related to pneumonia he developed while staying at Laurel Canyon Hospital in Los Angeles.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py -- --merged_model /home/ubuntu/merged-ul2-model-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb454b6-4c26-43a1-9b95-cb883d08c583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6dcdeb-a857-4845-8bf8-39fd9dd5f679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
