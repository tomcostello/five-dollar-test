{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b899393e-4b89-4c7a-aaec-d65d56ba7520",
   "metadata": {},
   "source": [
    "# First We Install Some Libraries\n",
    "The basic library is transformrers.  We also need accelerate for some reason. This takes 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d9c59f-db7c-4cac-a48d-8296cdaff37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.8/site-packages (23.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install \"numexpr==2.7.3\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9866f3c-9c61-4b54-9614-f14432a51e78",
   "metadata": {},
   "source": [
    "### Downloading the model from HuggingFace\n",
    "This loads the Flan-UL2 model and its tokenizer.  This will download the model from Huggngface so will take 15 minutes as the model is 40GB. The next time we run this, it will be cached. The second time we run it will be faster, about 25 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77475b8c-fece-4e52-a6b8-f00e4b2c48f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126fdf7e494f4d4da893b146838d22de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\")     \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b0679-0211-4aea-8379-055fd388e352",
   "metadata": {},
   "source": [
    "## Our generate function\n",
    "This is a function that, given a query, generates a response from the LLM. This is for testing. Later we will turn this into a Streamlit app.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b370253-f25a-4d72-9187-f82aa0228a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(str):\n",
    "    inputs = tokenizer(str, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs, max_length=500,\n",
    "                             num_beams=2,\n",
    "                             repetition_penalty=2.5,\n",
    "                             length_penalty=1.0,\n",
    "                             early_stopping=True,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             use_cache=True,\n",
    "                             do_sample = True,\n",
    "                             temperature = 1.5,\n",
    "                             top_k = 50,\n",
    "                             top_p = 0.95)\n",
    "    \n",
    "    print(tokenizer.decode(outputs[0][1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9927bf2-6faf-44d0-a561-522dfc8e3761",
   "metadata": {},
   "source": [
    "## Testing the original Flan-UL2\n",
    "We now test if the model works.  This is tuned to answer questions with short responses, so it gives just a few words as the answer.  We use a pretty high temperature so the output often changes.  \n",
    "\n",
    "Sometimes it says *record sales* or *singer songwriter* or *he had long and bizarre hair* which I suppose are arguably correct. These answers are really short, and we are going to fine tune things to make it give long, web-page like answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71eb63b-9b32-4820-af5d-d7446afd8c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for his music\n"
     ]
    }
   ],
   "source": [
    "f(\"How did David Bowie become famous?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923f60f-bf8c-4659-aa55-661e0f1c3308",
   "metadata": {},
   "source": [
    "## Install Peft and other libraries for fine tunung\n",
    "\n",
    "Now we install peft, the package that allows parameter efficient fine tuning.  We will also need datasets (to load a dataset). This takes 12 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ef7aed-da5d-45d8-84b1-4c5223abda08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install \"peft==0.2.0\" --quiet\n",
    "!pip install \"transformers==4.27.2\"  \"accelerate==0.17.1\" \"evaluate==0.4.0\" loralib  --quiet\n",
    "!pip install \"datasets==2.9.0\" --quiet\n",
    "\n",
    "!pip install protobuf==\"3.20.*\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752cb4b-8cab-4ef3-a300-c1a3540e427f",
   "metadata": {},
   "source": [
    "## Where we keep things\n",
    "\n",
    "We set a bunch of variables so we have names of where we keep some files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f693820c-7f9a-45b4-b657-7ec5c455a6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /home/ubuntu/five-dollar-test/what.json /home/ubuntu/five-dollar-test/how.json >  /home/ubuntu/five-dollar-test/train.json\n",
    "training_data = \"/home/ubuntu/five-dollar-test/train.json\" # The training data is taken from this file.\n",
    "shuffled_data = \"/home/ubuntu/data/train.json\" # We shuffle the data and put it here.\n",
    "tokenized_data = \"/home/ubuntu/data-all-test/train\"  # Where we put the tokenized data\n",
    "\n",
    "output_dir=\"/home/ubuntu/lora-flan-ul2-fact\"  # the place where we save the interim results\n",
    "\n",
    "peft_model_id=\"/home/ubuntu/results-ul2-1k\"  # The place where we save the peft model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c20db-14c7-446f-b82e-874a22c402b5",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "We load the data in, shuffle it, and write it out as tokens (so numbers). This takes about 50 seconds, so we save the data so we can reload it quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c5cd98-32ee-44b3-8e0b-fbbb3587e0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/ubuntu/data/ exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-09e93e05526e2545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/data to /home/ubuntu/.cache/huggingface/datasets/json/data-09e93e05526e2545/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1235b4184537484990da565631c3f1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b0dcd7d5b6449491344dae6bf6178a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd293967be234e3dbf5dbbd47c9a6595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/data-09e93e05526e2545/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1ed867477e419c96e9e2ed4bb51ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e91bb36a61748d9966664dccce8d00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ca50a27c27435590e31aef4f01e995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "jjout = []\n",
    "with open(training_data ) as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        i = json.loads(line)\n",
    "        if not i.get(\"input\") or not i.get(\"output\"):\n",
    "            continue\n",
    "        jjout.append({\"input\":  i[\"input\"], \"output\": i[\"output\"]})\n",
    "        \n",
    "import random\n",
    "random.shuffle(jjout)  # randomly shuffle the data.\n",
    "\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(\"/home/ubuntu/data/\")\n",
    "except:\n",
    "    print(\"Directory /home/ubuntu/data/ exists\")\n",
    "\n",
    "with open(shuffled_data, \"w\") as file:\n",
    "    file.write(json.dumps(jjout))\n",
    "\n",
    "from datasets import load_dataset\n",
    "data_files = {\"train\": \"train.json\"}\n",
    "dataset = load_dataset(\"/home/ubuntu/data/\", data_files=data_files)\n",
    "\n",
    "max_length = 512 #tokenizer.model_max_length  Flan-ul2 has this set to 2048 which makes things a little slow.  If we used flash attention this would be quicker.\n",
    "\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    inputs = [f\"{{input}}\\n\".format(input=item) for item in sample['input']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=padding, truncation=True)\n",
    "    labels = tokenizer(text_target=sample['output'], max_length=max_length, padding=padding, truncation=True)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))\n",
    "tokenized_dataset[\"train\"].save_to_disk(tokenized_data )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffad8da-fb08-4321-8040-5083011751aa",
   "metadata": {},
   "source": [
    "## Load back in the datset\n",
    "\n",
    "We load our dataset from disk.  This is mostly so we can rerun things without regenerating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ada9b77-925d-46e3-b00c-2ae9066ef3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469abc50-cfa8-43f0-9de6-c8a106ccc0a4",
   "metadata": {},
   "source": [
    "## Making the LoRA model\n",
    "\n",
    "Now we make the lora model which is a wrapper around the base model. This takes 25 seconds.  Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a60ef5-3039-4087-a3d4-dd30b95ba2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Trainable Parameters!!\n",
      "trainable params: 12582912 || all params: 19472196608 || trainable%: 0.06461988985274732\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    " r=8,  # the rank.  This is the smaller dimension of the two d*r and r*d matrices that multiply to make a d*d matrix.\n",
    " lora_alpha=32,  # this is the mixing factor.  32 is quite high.\n",
    " target_modules=[\"q\", \"v\"],  # We just replace the value and query matrices that are used in attention. The choices are  [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"].\n",
    " lora_dropout=0.05,  # This is a regularization thing to make the model train better.  We ignore 5% of the parameters each time to encourage the others.\n",
    " bias=\"none\", # This can be none, lora_only, or all.  none seems the safe choice.\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "import torch.nn as nn\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # This makes things faster\n",
    "model.enable_input_require_grads() # This computes the gradients for the input embeddings so we can fine tune.\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)  # we loaded the base model in bf16 to we need to cast the top to float so we can peft it.\n",
    "model.config.use_cache = False  # You can't use the cache with gradient checkpointing.  \n",
    "\n",
    "print(\"The Trainable Parameters!!\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130c0cf-c3ba-4457-a3f5-3c0481af3f75",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf3655-d1a5-461b-a8b3-415f5c36cc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0399db37-816f-4281-a1d5-1e88ee3ceb03",
   "metadata": {},
   "source": [
    "# The Training Parameters\n",
    "\n",
    "These are what controls the trainer. The data collator helps load the data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43cce517-cc0b-4e06-8cd6-1ee710d0e5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8, # We train 8 examples at a time.\n",
    "    gradient_accumulation_steps=1,  # We don't accumulate gradients\n",
    "    #auto_find_batch_size=True,  # This sometimes works, and can replace the previous two lines, but gave me out of mempoery errors once.\n",
    "    learning_rate=1e-3, # higher learning rate,  Learning rates are usually 10 or 100 times smaller than this, but this seems to work.\n",
    "    num_train_epochs=1, # We train for 1 epoch, but actually we stop after max_steps so this will not be ineffect unless you have less than 1000 examples.\n",
    "    logging_dir=f\"{output_dir}/logs\", # Where we log things\n",
    "    logging_strategy=\"steps\", # We log by steps.  epoch would loh once every epoch and no turns off logging.\n",
    "    logging_steps=50,  # we log every 50 steps to see how things are going.\n",
    "    max_steps=250, # take 250 steps of 8 examples, so 1000 examples in total.\n",
    "    bf16=True,  # We train in bf16 as fp16 can overflow in T5 models.  bf16 has more range but less precision.\n",
    "    save_strategy=\"no\", #\n",
    "    report_to=\"tensorboard\",  # This makes our little progress bar\n",
    "    optim='adamw_torch',  # this is the AdamW optimizer as the default one is deprecated.\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00193d6-fca5-4092-b0a9-3329f92bcdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df83d23-6290-457d-ba69-d54b0508a172",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "This takes 10 minutes or so.  If you make max_steps bigger, it will take longer.  Quality will change, but 1000 samples seems to work well.\n",
    "Remember we are trying to teach it a style, not to teach it facts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5945f7ae-d58e-40ac-bb7e-6f8212cca9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 08:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.226700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=2.227068145751953, metrics={'train_runtime': 536.3496, 'train_samples_per_second': 3.729, 'train_steps_per_second': 0.466, 'total_flos': 1.18828642074624e+17, 'train_loss': 2.227068145751953, 'epoch': 0.24})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01670c0-2e98-4837-a875-27b7da58b019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae2a62e6-383d-4fa5-b403-b8e37feff636",
   "metadata": {},
   "source": [
    "## Save the Peft Model\n",
    "This saves the peft model in results-ul2-1k or whatever you set *peft_model_id* to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "216d3b4e-381b-4de0-a4fc-62139503a563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/results-ul2-1k/tokenizer_config.json',\n",
       " '/home/ubuntu/results-ul2-1k/special_tokens_map.json',\n",
       " '/home/ubuntu/results-ul2-1k/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced882f-f78b-444b-a7a8-5c0fa5a8fefb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge the Peft model with the original model\n",
    "We merge the models together so that inference will be faster. This takes about two minutes.   Once this is done, we restart the kernel.  This will prevent us running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8337ec5b-a7dc-43de-9199-e798ed16af70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba05c50165af453dbd9f7d49f2a60da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "tokenizer\n",
      "lora\n",
      "eval\n",
      "merge\n",
      "input sentence: does the white house have a dining room\n",
      "------------------------------\n",
      "Answer:\n",
      "The White House is the official residence of the President of United States. It is located at 1600 Pennsylvania Avenue in Washington, D.C., across from the U.S. Capitol and next door to the Vice President's residence. The name \"White House\" derives from its white-brick exterior. There are a number of different rooms in the house, including the Oval Office, which is used by the president for meetings with other heads of state and foreign dignitaries. Other rooms include the Executive Office Building (EOB), where the executive branch of government operates, and the West Wing, home to most cabinet members and senior administration officials. In addition, there are several smaller buildings on the grounds, such as the Press Room, the Diplomatic Reception Hall, Secret Service headquarters, Rose Garden, South Lawn, Equestrian Center, National Arboretum, St. Elizabeths Hospital, Marine Barracks, Naval Observatory, Smithsonian Institution, Mount Vernon, Air Force One, Lincoln Memorial, John F. Kennedy Presidential Library and Museum, Arlington National Cemetery, Camp David, Bethesda Naval Base, Fort Myers, Florida, Chesapeake Bay Environmental Research Laboratory, Joint Base Anacostia-Bolling, Maryland, Navy Annex, Norfolk, Virginia, Coast Guard Headquarters, Pentagon, Department of Veterans Affairs, Central Intelligence Agency, Defense Information School, FBI Academy, Bureau of Alcohol, Tobacco, Firearms and Explosives, Federal Aviation Administration, US Postal Service, Supreme Court, Trump International Hotel and Tower, Mar-a Lago Resort, New York City, N.Y.\n",
      "reset\n",
      "input sentence: does the white house have a dining room\n",
      "------------------------------\n",
      "Answer:\n",
      "This is the dining room of the White House. It's where President Trump and First Lady Melania Trump eat most of their meals. The table is set with china, silverware, and glassware from the National Museum of American History in Washington D.C.\n",
      "saving\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16,  device_map={\"\":0})\n",
    "print(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\" )\n",
    "print(\"tokenizer\")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "print(\"lora\")\n",
    "model.eval()\n",
    "print(\"merge\")\n",
    "\n",
    "import peft \n",
    "key_list = [key for key, _ in model.base_model.model.named_modules() if \"lora\" not in key]\n",
    "for key in key_list:\n",
    "    parent, target, target_name = model.base_model._get_submodules(key)\n",
    "    if isinstance(target, peft.tuners.lora.Linear):\n",
    "        bias = target.bias is not None\n",
    "        new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n",
    "        model.base_model._replace_module(parent, target_name, new_module, target)\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "print(\"reset\")\n",
    "model = model.base_model.model\n",
    "print(\"saving\")\n",
    "model.save_pretrained(\"/home/ubuntu/merged-ul2-model-v4\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48819764-6207-437e-a77e-813eb05ce055",
   "metadata": {},
   "source": [
    "## Start our Streamlit App\n",
    "\n",
    "First, we restart the kernel, choosing \"Restart Kernel\" from the Kernel menu above.  This is to save GPU memory, which we will need for the Streamlit app.  Then we start the streamlit app with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf1c156-eb87-4799-af36-fd1517481998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.29.150.213:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://209.20.159.16:8501\u001b[0m\n",
      "\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:26<00:00,  6.51s/it]\n",
      "2023-06-02 21:39:56.472895: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 21:39:56.650533: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: No preset parameters were found for the device that Open MPI\n",
      "detected:\n",
      "\n",
      "  Local host:            209-20-159-16\n",
      "  Device name:           mlx5_0\n",
      "  Device vendor ID:      0x02c9\n",
      "  Device vendor part ID: 4122\n",
      "\n",
      "Default device parameters will be used, which may result in lower\n",
      "performance.  You can edit any of the files specified by the\n",
      "btl_openib_device_param_files MCA parameter to set values for your\n",
      "device.\n",
      "\n",
      "NOTE: You can turn off this warning by setting the MCA parameter\n",
      "      btl_openib_warn_no_device_params_found to 0.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           209-20-159-16\n",
      "  Local device:         mlx5_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       udcm\n",
      "--------------------------------------------------------------------------\n",
      "input sentence: can breast cancer go away\n",
      "------------------------------\n",
      "\n",
      "2023-06-02 21:40:15,350\n",
      "\n",
      "            Question: can breast cancer go away\n",
      "            Answer: Answer:\n",
      "Breast cancer is one of the most common types of cancer in women. There are two kinds of breast cancer: benign and malignant. In this article, we'll explore the differences between them and explain how they might change over time. Although a lot of things can affect your risk of getting breast Cancer, genetics is not an accurate indicator for whether you will get it or not. A person's likelihood of developing breast tissue cancer has nothing to do with her family history. The majority of people who get breast tumors have no family members with any form of early-onset breast disease. For example, only 1 out of every 100 women diagnosed with breast lumps will die from the condition. But many other health conditions can increase your chances of being diagnosed by 2 to 5 times. These include: *: Women under 60 years old are more likely to be diagnosed than women older than that. They also have less chance of receiving treatment because their symptoms may not be severe enough to qualify for medical coverage. *; Women with high blood pressure, diabetes, obesity, menopause, and certain fertility issues are at increased risk. If you already have one type of BRCA gene mutation, you're 10 times more like to develop another kind of mutation. And if you don't know which ones you have, then there could be multiple alleles present. One of these mutations makes it hard for your cells to make the protein called p53. P53 helps keep cell growth under control. Without it, your body begins to divide uncontrollably. You may see unusual changes in your breasts during your lifetime. This happens because hormones (the chemicals that help regulate puberty) and birth control pills can interfere with normal cell division. Some hormonal therapies used to treat hyperactive thyroid glands can raise your estrogen levels. Your doctor may recommend switching to non-hormonal therapy after diagnosis. Hormonal therapies can also cause serious complications such as bone loss, nerve damage, heart problems, vaginal bleeding, pregnancy, miscarriage, male infertility, muscle weakness, depression, anxiety, hair loss/hair thinning, liver dysfunction, joint pain, vomiting, dizziness, fatigue, insomnia, headaches, sweating, nausea, hot flashes\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py -- --merged_model /home/ubuntu/merged-ul2-model-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb454b6-4c26-43a1-9b95-cb883d08c583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6dcdeb-a857-4845-8bf8-39fd9dd5f679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
