{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b899393e-4b89-4c7a-aaec-d65d56ba7520",
   "metadata": {},
   "source": [
    "# First We Install Some Libraries\n",
    "The basic library is transformrers.  We also need accelerate for some reason. This takes 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d9c59f-db7c-4cac-a48d-8296cdaff37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.8/site-packages (23.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install \"numexpr==2.7.3\" --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bc3fce-cd61-453d-a219-b27257767a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade fsspec --quiet\n",
    "!pip install \"datasets==2.9.0\" --quiet\n",
    "!pip install streamlit --quiet\n",
    "!pip install protobuf==\"3.20.*\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9866f3c-9c61-4b54-9614-f14432a51e78",
   "metadata": {},
   "source": [
    "### Downloading the model from HuggingFace\n",
    "This loads the Flan-UL2 model and its tokenizer.  This will download the model from Huggngface so will take 15 minutes as the model is 40GB. The next time we run this, it will be cached. The second time we run it will be faster, about 25 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77475b8c-fece-4e52-a6b8-f00e4b2c48f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e567de4f318246d2a8f091a2800bb2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/784 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1add7f814f4a929ecad05567ca5848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/67.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08b646047324f2b805eb0870b3c71d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb2f03a594145b0abfa7855bfcacb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00008.bin:   0%|          | 0.00/4.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63f96ab961d4e668190e262c01b968e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00008.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e62bc57830b4b4eb35cb3e44355a96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00008.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1688a9b38124f5b961ccca07a69091e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00008.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e045bbb318a412cae7399646ae515c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00008.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c68e7fa7ee049dd9f5f81d8b40212d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00008.bin:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23c2105c971486a82357a0041fa2145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00008.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0bca8042624d46b0e1a49cdc490908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00008.bin:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbab1edf1d424b838fef69503f80a00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52df65d65c040b2bb90c97ddffd4008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d0673af32343daaeed4958481e7b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339cf70c6e134a41988d8324ece25095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151820674b054ecc9afcf2699b1e719c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733cac0720df47f7b01cf00eaede913d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\")     \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b0679-0211-4aea-8379-055fd388e352",
   "metadata": {},
   "source": [
    "## Our generate function\n",
    "This is a function that, given a query, generates a response from the LLM. This is for testing. Later we will turn this into a Streamlit app.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b370253-f25a-4d72-9187-f82aa0228a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(str):\n",
    "    inputs = tokenizer(str, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs, max_length=500,\n",
    "                             num_beams=2,\n",
    "                             repetition_penalty=2.5,\n",
    "                             length_penalty=1.0,\n",
    "                             early_stopping=True,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             use_cache=True,\n",
    "                             do_sample = True,\n",
    "                             temperature = 1.5,\n",
    "                             top_k = 50,\n",
    "                             top_p = 0.95)\n",
    "    \n",
    "    print(tokenizer.decode(outputs[0][1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9927bf2-6faf-44d0-a561-522dfc8e3761",
   "metadata": {},
   "source": [
    "## Testing the original Flan-UL2\n",
    "We now test if the model works.  This is tuned to answer questions with short responses, so it gives just a few words as the answer.  We use a pretty high temperature so the output often changes.  \n",
    "\n",
    "Sometimes it says *record sales* or *singer songwriter* or *he had long and bizarre hair* which I suppose are arguably correct. These answers are really short, and we are going to fine tune things to make it give long, web-page like answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71eb63b-9b32-4820-af5d-d7446afd8c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singer\n"
     ]
    }
   ],
   "source": [
    "f(\"How did David Bowie become famous?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923f60f-bf8c-4659-aa55-661e0f1c3308",
   "metadata": {},
   "source": [
    "## Install Peft and other libraries for fine tunung\n",
    "\n",
    "Now we install peft, the package that allows parameter efficient fine tuning.  We will also need datasets (to load a dataset). This takes 12 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ef7aed-da5d-45d8-84b1-4c5223abda08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install \"peft==0.2.0\" --quiet\n",
    "!pip install \"transformers==4.27.2\"  \"accelerate==0.17.1\" \"evaluate==0.4.0\" loralib  --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752cb4b-8cab-4ef3-a300-c1a3540e427f",
   "metadata": {},
   "source": [
    "## Where we keep things\n",
    "\n",
    "We set a bunch of variables so we have names of where we keep some files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f693820c-7f9a-45b4-b657-7ec5c455a6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /home/ubuntu/five-dollar-test/what.json /home/ubuntu/five-dollar-test/how.json >  /home/ubuntu/five-dollar-test/train.json\n",
    "training_data = \"/home/ubuntu/five-dollar-test/train.json\" # The training data is taken from this file.\n",
    "shuffled_data = \"/home/ubuntu/data/train.json\" # We shuffle the data and put it here.\n",
    "tokenized_data = \"/home/ubuntu/data-all-test/train\"  # Where we put the tokenized data\n",
    "\n",
    "output_dir=\"/home/ubuntu/lora-flan-ul2-fact\"  # the place where we save the interim results\n",
    "\n",
    "peft_model_id=\"/home/ubuntu/results-ul2-1k\"  # The place where we save the peft model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c20db-14c7-446f-b82e-874a22c402b5",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "We load the data in, shuffle it, and write it out as tokens (so numbers). This takes about 50 seconds, so we save the data so we can reload it quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c5cd98-32ee-44b3-8e0b-fbbb3587e0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-aa940360b9385bdc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/data to /home/ubuntu/.cache/huggingface/datasets/json/data-aa940360b9385bdc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2824e0231a2e4a8981cc9384256f8848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548259d6b6e84ff1b56d5f3941bf38b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba20d58a793441a0b07f82d804eee275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/data-aa940360b9385bdc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08d9fdb147a4462a03848df072b70ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e055612a2fd4658886ed3adcf29ee32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c6cae6b2ff439890f878d8acad39ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "jjout = []\n",
    "with open(training_data ) as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        i = json.loads(line)\n",
    "        if not i.get(\"input\") or not i.get(\"output\"):\n",
    "            continue\n",
    "        jjout.append({\"input\":  i[\"input\"], \"output\": i[\"output\"]})\n",
    "        \n",
    "import random\n",
    "random.shuffle(jjout)  # randomly shuffle the data.\n",
    "\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(\"/home/ubuntu/data/\")\n",
    "except:\n",
    "    print(\"Directory /home/ubuntu/data/ exists\")\n",
    "\n",
    "with open(shuffled_data, \"w\") as file:\n",
    "    file.write(json.dumps(jjout))\n",
    "\n",
    "from datasets import load_dataset\n",
    "data_files = {\"train\": \"train.json\"}\n",
    "dataset = load_dataset(\"/home/ubuntu/data/\", data_files=data_files)\n",
    "\n",
    "max_length = 512 #tokenizer.model_max_length  Flan-ul2 has this set to 2048 which makes things a little slow.  If we used flash attention this would be quicker.\n",
    "\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    inputs = [f\"{{input}}\\n\".format(input=item) for item in sample['input']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=padding, truncation=True)\n",
    "    labels = tokenizer(text_target=sample['output'], max_length=max_length, padding=padding, truncation=True)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))\n",
    "tokenized_dataset[\"train\"].save_to_disk(tokenized_data )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffad8da-fb08-4321-8040-5083011751aa",
   "metadata": {},
   "source": [
    "## Load back in the datset\n",
    "\n",
    "We load our dataset from disk.  This is mostly so we can rerun things without regenerating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ada9b77-925d-46e3-b00c-2ae9066ef3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469abc50-cfa8-43f0-9de6-c8a106ccc0a4",
   "metadata": {},
   "source": [
    "## Making the LoRA model\n",
    "\n",
    "Now we make the lora model which is a wrapper around the base model. This takes 25 seconds.  Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a60ef5-3039-4087-a3d4-dd30b95ba2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Trainable Parameters!!\n",
      "trainable params: 12582912 || all params: 19472196608 || trainable%: 0.06461988985274732\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    " r=8,  # the rank.  This is the smaller dimension of the two d*r and r*d matrices that multiply to make a d*d matrix.\n",
    " lora_alpha=32,  # this is the mixing factor.  32 is quite high.\n",
    " target_modules=[\"q\", \"v\"],  # We just replace the value and query matrices that are used in attention. The choices are  [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"].\n",
    " lora_dropout=0.05,  # This is a regularization thing to make the model train better.  We ignore 5% of the parameters each time to encourage the others.\n",
    " bias=\"none\", # This can be none, lora_only, or all.  none seems the safe choice.\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "import torch.nn as nn\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # This makes things faster\n",
    "model.enable_input_require_grads() # This computes the gradients for the input embeddings so we can fine tune.\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)  # we loaded the base model in bf16 to we need to cast the top to float so we can peft it.\n",
    "model.config.use_cache = False  # You can't use the cache with gradient checkpointing.  \n",
    "\n",
    "print(\"The Trainable Parameters!!\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130c0cf-c3ba-4457-a3f5-3c0481af3f75",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf3655-d1a5-461b-a8b3-415f5c36cc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0399db37-816f-4281-a1d5-1e88ee3ceb03",
   "metadata": {},
   "source": [
    "# The Training Parameters\n",
    "\n",
    "These are what controls the trainer. The data collator helps load the data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43cce517-cc0b-4e06-8cd6-1ee710d0e5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8, # We train 8 examples at a time.\n",
    "    gradient_accumulation_steps=1,  # We don't accumulate gradients\n",
    "    #auto_find_batch_size=True,  # This sometimes works, and can replace the previous two lines, but gave me out of mempoery errors once.\n",
    "    learning_rate=1e-3, # higher learning rate,  Learning rates are usually 10 or 100 times smaller than this, but this seems to work.\n",
    "    num_train_epochs=1, # We train for 1 epoch, but actually we stop after max_steps so this will not be ineffect unless you have less than 1000 examples.\n",
    "    logging_dir=f\"{output_dir}/logs\", # Where we log things\n",
    "    logging_strategy=\"steps\", # We log by steps.  epoch would loh once every epoch and no turns off logging.\n",
    "    logging_steps=50,  # we log every 50 steps to see how things are going.\n",
    "    max_steps=250, # take 250 steps of 8 examples, so 1000 examples in total.\n",
    "    bf16=True,  # We train in bf16 as fp16 can overflow in T5 models.  bf16 has more range but less precision.\n",
    "    save_strategy=\"no\", #\n",
    "    report_to=\"tensorboard\",  # This makes our little progress bar\n",
    "    optim='adamw_torch',  # this is the AdamW optimizer as the default one is deprecated.\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00193d6-fca5-4092-b0a9-3329f92bcdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df83d23-6290-457d-ba69-d54b0508a172",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "This takes 10 minutes or so.  If you make max_steps bigger, it will take longer.  Quality will change, but 1000 samples seems to work well.\n",
    "Remember we are trying to teach it a style, not to teach it facts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5945f7ae-d58e-40ac-bb7e-6f8212cca9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 08:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.226000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=2.2434981384277344, metrics={'train_runtime': 536.1627, 'train_samples_per_second': 3.73, 'train_steps_per_second': 0.466, 'total_flos': 1.18828642074624e+17, 'train_loss': 2.2434981384277344, 'epoch': 0.24})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01670c0-2e98-4837-a875-27b7da58b019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae2a62e6-383d-4fa5-b403-b8e37feff636",
   "metadata": {},
   "source": [
    "## Save the Peft Model\n",
    "This saves the peft model in results-ul2-1k or whatever you set *peft_model_id* to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "216d3b4e-381b-4de0-a4fc-62139503a563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/results-ul2-1k/tokenizer_config.json',\n",
       " '/home/ubuntu/results-ul2-1k/special_tokens_map.json',\n",
       " '/home/ubuntu/results-ul2-1k/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced882f-f78b-444b-a7a8-5c0fa5a8fefb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge the Peft model with the original model\n",
    "We merge the models together so that inference will be faster. This takes about two minutes.   Once this is done, we restart the kernel.  This will prevent us running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2c95f59-59d5-40f3-b733-0c784ae03915",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import peft \n",
    "key_list = [key for key, _ in model.base_model.model.named_modules() if \"lora\" not in key]\n",
    "for key in key_list:\n",
    "    parent, target, target_name = model.base_model._get_submodules(key)\n",
    "    if isinstance(target, peft.tuners.lora.Linear):\n",
    "        bias = target.bias is not None\n",
    "        new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n",
    "        model.base_model._replace_module(parent, target_name, new_module, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8337ec5b-a7dc-43de-9199-e798ed16af70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddf9bbde57347799efdfc9210025ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "tokenizer\n",
      "lora\n",
      "merge\n",
      "reset\n",
      "saving\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "#from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "print(\"load model\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\")     \n",
    "\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16,  device_map={\"\":0})\n",
    "print(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\" )\n",
    "print(\"tokenizer\")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "print(\"lora\")\n",
    "model.eval()\n",
    "print(\"merge\")\n",
    "\n",
    "import peft \n",
    "key_list = [key for key, _ in model.base_model.model.named_modules() if \"lora\" not in key]\n",
    "for key in key_list:\n",
    "    parent, target, target_name = model.base_model._get_submodules(key)\n",
    "    if isinstance(target, peft.tuners.lora.Linear):\n",
    "        bias = target.bias is not None\n",
    "        new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n",
    "        model.base_model._replace_module(parent, target_name, new_module, target)\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "print(\"reset\")\n",
    "model = model.base_model.model\n",
    "print(\"saving\")\n",
    "model.save_pretrained(\"/home/ubuntu/merged-ul2-model-v4\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48819764-6207-437e-a77e-813eb05ce055",
   "metadata": {},
   "source": [
    "## Start our Streamlit App\n",
    "\n",
    "First, we restart the kernel, choosing \"Restart Kernel\" from the Kernel menu above.  This is to save GPU memory, which we will need for the Streamlit app.  Then we start the streamlit app with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1c156-eb87-4799-af36-fd1517481998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.29.150.237:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://209.20.158.247:8501\u001b[0m\n",
      "\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:23<00:00,  5.82s/it]\n",
      "2023-06-04 19:40:32.949626: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-04 19:40:33.153403: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: No preset parameters were found for the device that Open MPI\n",
      "detected:\n",
      "\n",
      "  Local host:            209-20-158-247\n",
      "  Device name:           mlx5_0\n",
      "  Device vendor ID:      0x02c9\n",
      "  Device vendor part ID: 4122\n",
      "\n",
      "Default device parameters will be used, which may result in lower\n",
      "performance.  You can edit any of the files specified by the\n",
      "btl_openib_device_param_files MCA parameter to set values for your\n",
      "device.\n",
      "\n",
      "NOTE: You can turn off this warning by setting the MCA parameter\n",
      "      btl_openib_warn_no_device_params_found to 0.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           209-20-158-247\n",
      "  Local device:         mlx5_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       udcm\n",
      "--------------------------------------------------------------------------\n",
      "input sentence: Does Ireland have large lakes\n",
      "------------------------------\n",
      "\n",
      "2023-06-04 19:40:44,494\n",
      "\n",
      "            Question: Does Ireland have large lakes\n",
      "            Answer: Answer:\n",
      "The lakes in Ireland are famous for their beautiful blue and green waters. But did you know that there are more than 5,000 lakes? It's the second-largest lake system in Europe, behind Lake Baikal in Siberia. And many of them are very large. We took a look at some of the largest lakes by area in Northern Ireland. Here are the five biggest: Lough Erne is the third-biggest lake in northern Ireland, stretching across 1,745 square miles. It is located to the east of Enniskillen in County Fermanagh. Slieve Dhu (also called Lunga Donegal) is another large lake with an area of 1,525 square km. This lake is south of Newry in county Down on the Irish Sea coast. You can read more about it here. Derryveigh is north of Dublin city centre and west of Clifden town. At just under 1,000 square kilometers, it is smaller than most of these lakes, but it has an interesting history. Read more here! In addition to those listed above, other lakes include: There are numerous smaller lakes too, such as:\n",
      "            \n",
      "input sentence: Can a giraffe laugh\n",
      "------------------------------\n",
      "\n",
      "2023-06-04 19:41:27,989\n",
      "\n",
      "            Question: Can a giraffe laugh\n",
      "            Answer: Answer:\n",
      "If youve never heard a giraffe laugh, it might not be what you expect. The most famous part of the animal is its neck, but theres more to them than that. Here are some things you probably didnt know about these tall creatures. We love hearing people laugh. Its one of our favorite sounds in the world. And when it comes to animals, nobody can quite beat the sound of their own unintentional giggles. This is especially true for humpback whales, elephants and cheetahs (which also seem to enjoy laughing). But while they may have the same sense of humor as humans, tigers and other mammals, those big-eyed beasts dont share another trait: They dont laugh like human beings do. In fact, no animal on Earth laughs at the moment we write this article. While pigs will occasionally emit an odd snort or grunt, both wild and domesticated hogs lack the ability to make noises similar to human laughter. As for cats, dogs and primates, they're all capable of vocalizing, though they tend to stick to meowing and barking. Of course, each species has its own unique take on how to express joy. Humans often find amusement in sharing jokes with friends and making funny faces. However, according to zoologist Dr. Jane Goodall, African bushbaby babies spend countless hours playing with toys and watching cartoons until they finally get bored and fall asleep. That doesnt mean they cant laugh; instead, researchers from the Australian University of Melbourne found that even baby bushbabies can react to visual cues by moving their tongues back and forth. Just like many other animals who use their mouths to speak, felines sometimes purr, growl and chatter. Cats are known for using their voices to show emotions, such as fear, happiness and anger. Scientists believe that \"purring\" is actually an attempt to communicate with others, rather than any sort of physical movement. According to studies published in Animal Cognition and Behaviour, female cat owners appear to be more likely to allow their cats to watch television programs because they want them to feel like they belong to the family. A study published earlier this year showed that male dog owners were less willing\n",
      "            \n",
      "input sentence: Does cheese make you sad\n",
      "------------------------------\n",
      "\n",
      "2023-06-04 19:42:17,336\n",
      "\n",
      "            Question: Does cheese make you sad\n",
      "            Answer: Answer:\n",
      "Cheese is a good source of calcium and protein, as well as vitamin D. It also contains many other nutrients such as zinc, folate, copper, and phosphorus. If you are pregnant or breastfeeding, you should talk to your doctor before eating more cheese than it is recommended. Cheese can make you sick if you eat too much in one sitting. You may feel nauseated, have stomach cramps, diarrhea, or vomiting. These symptoms usually go away on their own without treatment. But sometimes they do not, especially for older adults. Older adults with chronic health conditions might be at risk for serious illness from eating too little dairy. The amount of cheese you need depends on whether you: Pregnant or breast feeding: Eat 2 to 3 servings of dairy per day. This includes milk, yogurt, cheese, butter, cream, half-and-half, custard, pudding, margarine, etc. Vegetarian or vegan: Consume 2.5 to 4 cups of plant-based dairy products daily. (This includes non-dairy versions of the foods listed above) Adults over 50 years old: consume up to 8 cups (1,500ml) of total dairy every week. To get that amount, include low-fat options like almond milk and soymilk, along with lactose-free alternatives like coconut milk. *: Milk makes you hungry because it increases your blood sugar levels. Your body needs to produce extra insulin to process the food, which causes you to feel full. Try cutting out 1 cup of milk each day to see unless it helps reduce your hunger. However, don't cut out milk entirely. Dairy plays an important role in keeping your bones healthy. Studies show that women who drink milk regularly have lower rates of osteoporosis later in life. Lactobacillus acidophilus is an organism that lives in your intestines. They help break down food into components your body can absorb. One reason people become deficient in this bacteria is when they start taking antibiotics after being sick. Antibiotics kill the good bacteria in addition to the bad bacteria. Some studies suggest that having less bacteria leads to weight gain and digestive problems. For most people, dairy provides about 9 percent of their daily calcium intake. A diet high in dairy can lead to bone fractures and soft tissue injuries. People who suffer from diabetes, hypogly\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "\n",
    "!streamlit run app.py -- --merged_model /home/ubuntu/merged-ul2-model-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb454b6-4c26-43a1-9b95-cb883d08c583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6dcdeb-a857-4845-8bf8-39fd9dd5f679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
