{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b899393e-4b89-4c7a-aaec-d65d56ba7520",
   "metadata": {},
   "source": [
    "# First We Install Some Libraries\n",
    "The basic library is transformrers.  We also need accelerate for some reason. This takes 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d9c59f-db7c-4cac-a48d-8296cdaff37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.8/site-packages (23.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install \"numexpr==2.7.3\" --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bc3fce-cd61-453d-a219-b27257767a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade fsspec --quiet\n",
    "!pip install \"datasets==2.9.0\" --quiet\n",
    "!pip install streamlit --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9866f3c-9c61-4b54-9614-f14432a51e78",
   "metadata": {},
   "source": [
    "### Downloading the model from HuggingFace\n",
    "This loads the Flan-UL2 model and its tokenizer.  This will download the model from Huggngface so will take 15 minutes as the model is 40GB. The next time we run this, it will be cached. The second time we run it will be faster, about 25 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77475b8c-fece-4e52-a6b8-f00e4b2c48f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d032d02be94efa9aed9979c8dfe0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\")     \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b0679-0211-4aea-8379-055fd388e352",
   "metadata": {},
   "source": [
    "## Our generate function\n",
    "This is a function that, given a query, generates a response from the LLM. This is for testing. Later we will turn this into a Streamlit app.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b370253-f25a-4d72-9187-f82aa0228a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(str):\n",
    "    inputs = tokenizer(str, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs, max_length=500,\n",
    "                             num_beams=2,\n",
    "                             repetition_penalty=2.5,\n",
    "                             length_penalty=1.0,\n",
    "                             early_stopping=True,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             use_cache=True,\n",
    "                             do_sample = True,\n",
    "                             temperature = 1.5,\n",
    "                             top_k = 50,\n",
    "                             top_p = 0.95)\n",
    "    \n",
    "    print(tokenizer.decode(outputs[0][1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9927bf2-6faf-44d0-a561-522dfc8e3761",
   "metadata": {},
   "source": [
    "## Testing the original Flan-UL2\n",
    "We now test if the model works.  This is tuned to answer questions with short responses, so it gives just a few words as the answer.  We use a pretty high temperature so the output often changes.  \n",
    "\n",
    "Sometimes it says *record sales* or *singer songwriter* or *he had long and bizarre hair* which I suppose are arguably correct. These answers are really short, and we are going to fine tune things to make it give long, web-page like answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71eb63b-9b32-4820-af5d-d7446afd8c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singer\n"
     ]
    }
   ],
   "source": [
    "f(\"How did David Bowie become famous?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923f60f-bf8c-4659-aa55-661e0f1c3308",
   "metadata": {},
   "source": [
    "## Install Peft and other libraries for fine tunung\n",
    "\n",
    "Now we install peft, the package that allows parameter efficient fine tuning.  We will also need datasets (to load a dataset). This takes 12 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ef7aed-da5d-45d8-84b1-4c5223abda08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.8/site-packages (2.9.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets) (2023.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.9.0\n",
      "    Uninstalling datasets-2.9.0:\n",
      "      Successfully uninstalled datasets-2.9.0\n",
      "Successfully installed datasets-2.12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install \"peft==0.2.0\" --quiet\n",
    "!pip install \"transformers==4.27.2\"  \"accelerate==0.17.1\" \"evaluate==0.4.0\" loralib  --quiet\n",
    "!pip install \"datasets\" --upgrade \n",
    "!pip install protobuf==\"3.20.*\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752cb4b-8cab-4ef3-a300-c1a3540e427f",
   "metadata": {},
   "source": [
    "## Where we keep things\n",
    "\n",
    "We set a bunch of variables so we have names of where we keep some files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f693820c-7f9a-45b4-b657-7ec5c455a6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /home/ubuntu/five-dollar-test/what.json /home/ubuntu/five-dollar-test/how.json >  /home/ubuntu/five-dollar-test/train.json\n",
    "training_data = \"/home/ubuntu/five-dollar-test/train.json\" # The training data is taken from this file.\n",
    "shuffled_data = \"/home/ubuntu/data/train.json\" # We shuffle the data and put it here.\n",
    "tokenized_data = \"/home/ubuntu/data-all-test/train\"  # Where we put the tokenized data\n",
    "\n",
    "output_dir=\"/home/ubuntu/lora-flan-ul2-fact\"  # the place where we save the interim results\n",
    "\n",
    "peft_model_id=\"/home/ubuntu/results-ul2-1k\"  # The place where we save the peft model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c20db-14c7-446f-b82e-874a22c402b5",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "We load the data in, shuffle it, and write it out as tokens (so numbers). This takes about 50 seconds, so we save the data so we can reload it quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c5cd98-32ee-44b3-8e0b-fbbb3587e0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/ubuntu/data/ exists\n",
      "Downloading and preparing dataset json/data to /home/ubuntu/.cache/huggingface/datasets/json/data-9246216f02afb171/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73650e4713e434f85e11e17e61856d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095a0396b88b4131a60b607ea1c545f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2836f8a3b4844d8a2bd8e5960e92203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/data-9246216f02afb171/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a413a3d03ae04cb99784375b474d7101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e43a98b9a140dd9b7a30365419c6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080752a69d93400795073845f18d1a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "jjout = []\n",
    "with open(training_data ) as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        i = json.loads(line)\n",
    "        if not i.get(\"input\") or not i.get(\"output\"):\n",
    "            continue\n",
    "        jjout.append({\"input\":  i[\"input\"], \"output\": i[\"output\"]})\n",
    "        \n",
    "import random\n",
    "random.shuffle(jjout)  # randomly shuffle the data.\n",
    "\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(\"/home/ubuntu/data/\")\n",
    "except:\n",
    "    print(\"Directory /home/ubuntu/data/ exists\")\n",
    "\n",
    "with open(shuffled_data, \"w\") as file:\n",
    "    file.write(json.dumps(jjout))\n",
    "\n",
    "from datasets import load_dataset\n",
    "data_files = {\"train\": \"train.json\"}\n",
    "dataset = load_dataset(\"/home/ubuntu/data/\", data_files=data_files)\n",
    "\n",
    "max_length = 512 #tokenizer.model_max_length  Flan-ul2 has this set to 2048 which makes things a little slow.  If we used flash attention this would be quicker.\n",
    "\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    inputs = [f\"{{input}}\\n\".format(input=item) for item in sample['input']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=padding, truncation=True)\n",
    "    labels = tokenizer(text_target=sample['output'], max_length=max_length, padding=padding, truncation=True)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))\n",
    "tokenized_dataset[\"train\"].save_to_disk(tokenized_data )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffad8da-fb08-4321-8040-5083011751aa",
   "metadata": {},
   "source": [
    "## Load back in the datset\n",
    "\n",
    "We load our dataset from disk.  This is mostly so we can rerun things without regenerating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ada9b77-925d-46e3-b00c-2ae9066ef3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469abc50-cfa8-43f0-9de6-c8a106ccc0a4",
   "metadata": {},
   "source": [
    "## Making the LoRA model\n",
    "\n",
    "Now we make the lora model which is a wrapper around the base model. This takes 25 seconds.  Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7a60ef5-3039-4087-a3d4-dd30b95ba2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Trainable Parameters!!\n",
      "trainable params: 12582912 || all params: 19472196608 || trainable%: 0.06461988985274732\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    " r=8,  # the rank.  This is the smaller dimension of the two d*r and r*d matrices that multiply to make a d*d matrix.\n",
    " lora_alpha=32,  # this is the mixing factor.  32 is quite high.\n",
    " target_modules=[\"q\", \"v\"],  # We just replace the value and query matrices that are used in attention. The choices are  [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"].\n",
    " lora_dropout=0.05,  # This is a regularization thing to make the model train better.  We ignore 5% of the parameters each time to encourage the others.\n",
    " bias=\"none\", # This can be none, lora_only, or all.  none seems the safe choice.\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "import torch.nn as nn\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # This makes things faster\n",
    "model.enable_input_require_grads() # This computes the gradients for the input embeddings so we can fine tune.\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)  # we loaded the base model in bf16 to we need to cast the top to float so we can peft it.\n",
    "model.config.use_cache = False  # You can't use the cache with gradient checkpointing.  \n",
    "\n",
    "print(\"The Trainable Parameters!!\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130c0cf-c3ba-4457-a3f5-3c0481af3f75",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf3655-d1a5-461b-a8b3-415f5c36cc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0399db37-816f-4281-a1d5-1e88ee3ceb03",
   "metadata": {},
   "source": [
    "# The Training Parameters\n",
    "\n",
    "These are what controls the trainer. The data collator helps load the data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43cce517-cc0b-4e06-8cd6-1ee710d0e5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8, # We train 8 examples at a time.\n",
    "    gradient_accumulation_steps=1,  # We don't accumulate gradients\n",
    "    #auto_find_batch_size=True,  # This sometimes works, and can replace the previous two lines, but gave me out of mempoery errors once.\n",
    "    learning_rate=1e-3, # higher learning rate,  Learning rates are usually 10 or 100 times smaller than this, but this seems to work.\n",
    "    num_train_epochs=1, # We train for 1 epoch, but actually we stop after max_steps so this will not be ineffect unless you have less than 1000 examples.\n",
    "    logging_dir=f\"{output_dir}/logs\", # Where we log things\n",
    "    logging_strategy=\"steps\", # We log by steps.  epoch would loh once every epoch and no turns off logging.\n",
    "    logging_steps=50,  # we log every 50 steps to see how things are going.\n",
    "    max_steps=250, # take 250 steps of 8 examples, so 1000 examples in total.\n",
    "    bf16=True,  # We train in bf16 as fp16 can overflow in T5 models.  bf16 has more range but less precision.\n",
    "    save_strategy=\"no\", #\n",
    "    report_to=\"tensorboard\",  # This makes our little progress bar\n",
    "    optim='adamw_torch',  # this is the AdamW optimizer as the default one is deprecated.\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00193d6-fca5-4092-b0a9-3329f92bcdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df83d23-6290-457d-ba69-d54b0508a172",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "This takes 10 minutes or so.  If you make max_steps bigger, it will take longer.  Quality will change, but 1000 samples seems to work well.\n",
    "Remember we are trying to teach it a style, not to teach it facts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5945f7ae-d58e-40ac-bb7e-6f8212cca9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 09:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.264100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.210800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=2.2288980407714845, metrics={'train_runtime': 545.8914, 'train_samples_per_second': 3.664, 'train_steps_per_second': 0.458, 'total_flos': 1.18828642074624e+17, 'train_loss': 2.2288980407714845, 'epoch': 0.24})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01670c0-2e98-4837-a875-27b7da58b019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae2a62e6-383d-4fa5-b403-b8e37feff636",
   "metadata": {},
   "source": [
    "## Save the Peft Model\n",
    "This saves the peft model in results-ul2-1k or whatever you set *peft_model_id* to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "216d3b4e-381b-4de0-a4fc-62139503a563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/results-ul2-1k/tokenizer_config.json',\n",
       " '/home/ubuntu/results-ul2-1k/special_tokens_map.json',\n",
       " '/home/ubuntu/results-ul2-1k/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced882f-f78b-444b-a7a8-5c0fa5a8fefb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge the Peft model with the original model\n",
    "We merge the models together so that inference will be faster. This takes about two minutes.   Once this is done, we restart the kernel.  This will prevent us running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8337ec5b-a7dc-43de-9199-e798ed16af70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bef0dc7d87042bca7abbc89291a4a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "tokenizer\n",
      "lora\n",
      "merge\n",
      "reset\n",
      "saving\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16,  device_map={\"\":0})\n",
    "print(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\" )\n",
    "print(\"tokenizer\")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "print(\"lora\")\n",
    "model.eval()\n",
    "print(\"merge\")\n",
    "\n",
    "import peft \n",
    "key_list = [key for key, _ in model.base_model.model.named_modules() if \"lora\" not in key]\n",
    "for key in key_list:\n",
    "    parent, target, target_name = model.base_model._get_submodules(key)\n",
    "    if isinstance(target, peft.tuners.lora.Linear):\n",
    "        bias = target.bias is not None\n",
    "        new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n",
    "        model.base_model._replace_module(parent, target_name, new_module, target)\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "print(\"reset\")\n",
    "model = model.base_model.model\n",
    "print(\"saving\")\n",
    "model.save_pretrained(\"/home/ubuntu/merged-ul2-model-v4\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48819764-6207-437e-a77e-813eb05ce055",
   "metadata": {},
   "source": [
    "## Start our Streamlit App\n",
    "\n",
    "First, we restart the kernel, choosing \"Restart Kernel\" from the Kernel menu above.  This is to save GPU memory, which we will need for the Streamlit app.  Then we start the streamlit app with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1c156-eb87-4799-af36-fd1517481998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.23.1-py2.py3-none-any.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/lib/python3/dist-packages (from streamlit) (4.0.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/lib/python3/dist-packages (from streamlit) (7.0)\n",
      "Requirement already satisfied: importlib-metadata<7,>=1.4 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (6.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (1.23.5)\n",
      "Requirement already satisfied: packaging<24,>=14.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (23.0)\n",
      "Requirement already satisfied: pandas<3,>=0.25 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (1.5.3)\n",
      "Requirement already satisfied: pillow<10,>=6.2.0 in /usr/lib/python3/dist-packages (from streamlit) (7.0.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (12.0.0)\n",
      "Collecting pympler<2,>=0.9 (from streamlit)\n",
      "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.4 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (2.28.2)\n",
      "Collecting rich<14,>=10.11.0 (from streamlit)\n",
      "  Downloading rich-13.4.1-py3-none-any.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tenacity<9,>=8.0.0 (from streamlit)\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting toml<2 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (4.5.0)\n",
      "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
      "  Downloading tzlocal-4.3-py3-none-any.whl (20 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gitpython!=3.1.19,<4,>=3 (from streamlit)\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydeck<1,>=0.1.dev5 (from streamlit)\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit) (6.2)\n",
      "Collecting watchdog (from streamlit)\n",
      "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.8/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from altair<6,>=4.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: toolz in /usr/lib/python3/dist-packages (from altair<6,>=4.0->streamlit) (0.9.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3->streamlit)\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from importlib-metadata<7,>=1.4->streamlit) (3.15.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas<3,>=0.25->streamlit) (2023.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2->streamlit) (1.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests<3,>=2.4->streamlit) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.4->streamlit) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests<3,>=2.4->streamlit) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.4->streamlit) (2019.11.28)\n",
      "Collecting markdown-it-py<3.0.0,>=2.2.0 (from rich<14,>=10.11.0->streamlit)\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from rich<14,>=10.11.0->streamlit) (2.14.0)\n",
      "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit)\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting backports.zoneinfo (from tzlocal<5,>=1.1->streamlit)\n",
      "  Downloading backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /usr/lib/python3/dist-packages (from validators<1,>=0.2->streamlit) (4.4.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit)\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/lib/python3/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (19.3.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (5.12.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /home/ubuntu/.local/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/lib/python3/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.15.5)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.11.0->streamlit)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: validators\n",
      "  Building wheel for validators (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19565 sha256=b35f43fc9c1bb29bc31fc82d538318a34a73158ad2a32112e38ff93b8a2d9ca7\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
      "Successfully built validators\n",
      "Installing collected packages: watchdog, validators, tzdata, toml, tenacity, smmap, pympler, mdurl, backports.zoneinfo, pytz-deprecation-shim, pydeck, markdown-it-py, gitdb, tzlocal, rich, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.0.1 backports.zoneinfo-0.2.1 gitdb-4.0.10 gitpython-3.1.31 markdown-it-py-2.2.0 mdurl-0.1.2 pydeck-0.8.1b0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 rich-13.4.1 smmap-5.0.0 streamlit-1.23.1 tenacity-8.2.2 toml-0.10.2 tzdata-2023.3 tzlocal-4.3 validators-0.20.0 watchdog-3.0.0\n",
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.29.150.163:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://209.20.159.163:8501\u001b[0m\n",
      "\u001b[0m\n",
      "2023-06-03 21:37:21.960 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/streamlit/runtime/caching/cache_utils.py\", line 260, in _get_or_create_cached_value\n",
      "    cached_result = cache.read_result(value_key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/streamlit/runtime/caching/cache_resource_api.py\", line 457, in read_result\n",
      "    raise CacheKeyNotFoundError()\n",
      "streamlit.runtime.caching.cache_errors.CacheKeyNotFoundError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/streamlit/runtime/caching/cache_utils.py\", line 308, in _handle_cache_miss\n",
      "    cached_result = cache.read_result(value_key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/streamlit/runtime/caching/cache_resource_api.py\", line 457, in read_result\n",
      "    raise CacheKeyNotFoundError()\n",
      "streamlit.runtime.caching.cache_errors.CacheKeyNotFoundError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 628, in _get_config_dict\n",
      "    resolved_config_file = cached_file(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 409, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ubuntu/merged-ul2-model-v4'. Use `repo_type` argument if needed.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/home/ubuntu/five-dollar-test/app.py\", line 59, in <module>\n",
      "    model = get_model()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/streamlit/runtime/caching/cache_utils.py\", line 209, in wrapper\n",
      "    return cached_func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/streamlit/runtime/caching/cache_utils.py\", line 238, in __call__\n",
      "    return self._get_or_create_cached_value(args, kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/streamlit/runtime/caching/cache_utils.py\", line 263, in _get_or_create_cached_value\n",
      "    return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/streamlit/runtime/caching/cache_utils.py\", line 317, in _handle_cache_miss\n",
      "    computed_value = self._info.func(*func_args, **func_kwargs)\n",
      "  File \"/home/ubuntu/five-dollar-test/app.py\", line 55, in get_model\n",
      "    return AutoModelForSeq2SeqLM.from_pretrained(model_name,torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 441, in from_pretrained\n",
      "    config, kwargs = AutoConfig.from_pretrained(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\", line 896, in from_pretrained\n",
      "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 573, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 649, in _get_config_dict\n",
      "    raise EnvironmentError(\n",
      "OSError: Can't load the configuration of '/home/ubuntu/merged-ul2-model-v4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ubuntu/merged-ul2-model-v4' is the correct path to a directory containing a config.json file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!streamlit run app.py -- --merged_model /home/ubuntu/merged-ul2-model-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb454b6-4c26-43a1-9b95-cb883d08c583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6dcdeb-a857-4845-8bf8-39fd9dd5f679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
